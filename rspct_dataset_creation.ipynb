{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from bz2 import BZ2File\n",
    "import lzma\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    print(f'downloading file from {url} ...')\n",
    "    local_filename = url.split('/')[-1]\n",
    "    try:\n",
    "        os.unlink(local_filename)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    cmd = f'wget {url} -O {local_filename}'\n",
    "    subprocess.call(cmd.split())\n",
    "    return local_filename\n",
    "\n",
    "def open_zipped_file(fn):\n",
    "    if fn.endswith('.bz2'):\n",
    "        return BZ2File(fn)\n",
    "    elif fn.endswith('.xz'):\n",
    "        return lzma.open(fn)\n",
    "    else:\n",
    "        raise Exception(f'unknown extension for file {fn}')\n",
    "\n",
    "keep_columns = {'id', 'created_utc', 'subreddit', 'title', \n",
    "                'selftext', 'score', 'created', 'author', \n",
    "                'distinguished', 'score'}\n",
    "        \n",
    "def get_cleaned_json(jsn):\n",
    "    return  {k:v for k,v in jsn.items() if k in keep_columns}\n",
    "\n",
    "\n",
    "\n",
    "def get_self_posts(zipped_file):\n",
    "    print('processing file...')\n",
    "    out = []\n",
    "    total = 0\n",
    "    bad_lines = 0\n",
    "    \n",
    "    for line in tqdm(zipped_file):\n",
    "        total += 1\n",
    "        try:\n",
    "            jsn = json.loads(line)\n",
    "\n",
    "            if jsn['is_self'] and len(jsn['selftext']) >= MIN_LINE_LENGTH and jsn['subreddit'] != 'AskReddit' \\\n",
    "                              and len(jsn['selftext']) <= MAX_LINE_LENGTH:      \n",
    "                jsn = get_cleaned_json(jsn)\n",
    "                out.append(jsn)\n",
    "        except Exception:\n",
    "            bad_lines+=1\n",
    "            continue\n",
    "    print(total, bad_lines)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def get_self_posts_from_url(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    out_filename = f\"/data/reddit/{local_filename}\"\n",
    "    out_filename = \".\".join(out_filename.split(\".\")[:-1]) + \".json\"\n",
    "    print(out_filename)\n",
    "    download_file(url)\n",
    "    \n",
    "    #local_filename = url.split('/')[-1]\n",
    "    with open_zipped_file(local_filename) as f:\n",
    "        self_posts = get_self_posts(f)\n",
    "    json.dump(self_posts, open(out_filename, 'w'))   \n",
    "    #os.unlink(local_filename)\n",
    "    \n",
    "    \n",
    "#url = 'https://files.pushshift.io/reddit/submissions/RS_2018-01.xz'\n",
    "#get_self_posts_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = ['/RS_2017-04.bz2',\n",
    "             '/RS_2017-01.bz2',\n",
    "             '/RS_2017-02.bz2',\n",
    "             '/RS_2017-03.bz2',\n",
    "             '/RS_2017-05.bz2',\n",
    "             '/RS_2017-06.bz2',\n",
    "             '/RS_2017-07.xz',\n",
    "             '/RS_2017-08.bz2',\n",
    "             '/RS_2017-09.bz2',\n",
    "             '/RS_2017-10.bz2',\n",
    "             '/RS_2017-11.xz',\n",
    "             '/RS_2017-12.xz']\n",
    "\n",
    "filenames += [f'/RS_2016-{str(i).zfill(2)}.bz2' for i in range(6, 13)]\n",
    "filenames += [f'/RS_2018-0{i}.xz' for i in range(1, 6)]\n",
    "\n",
    "\n",
    "urls = ['https://files.pushshift.io/reddit/submissions'+ s for s in filenames]\n",
    "\n",
    "\n",
    "bad_urls = []\n",
    "for url in urls:\n",
    "    try: \n",
    "        get_self_posts_from_url(url)\n",
    "    except:\n",
    "        raise\n",
    "        print(f\"BAD FILE! {url}\")\n",
    "        bad_urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleantext methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import string\n",
    "import re\n",
    "\n",
    "URL_REGEX = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "REPEATED_CHARACTER_REGEX = re.compile(r\"(([A-z])\\2{2,})\")\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "\n",
    "def remove_excess_whitespace_from_string(string):\n",
    "    string = string.strip()\n",
    "    return \" \".join(string.split())\n",
    "\n",
    "\n",
    "def remove_punctuation_from_string(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "\n",
    "def remove_html_from_string(string):\n",
    "    stripper = MLStripper()\n",
    "    stripper.feed(string)\n",
    "    return stripper.get_data()\n",
    "\n",
    "def replace_with_double_character(matchobj):\n",
    "    return matchobj.group(2) * 2\n",
    "\n",
    "\n",
    "def remove_repeated_alpha_chars(string):\n",
    "    \"\"\"\n",
    "    Looks for runs of characters of 3 or more of the same thing, and then replaces it with just 2\n",
    "    of that same character\n",
    "    (useful for user text e.g. twitter, with strings like 'oooohhhhhhhh noooooooo' ->  'oohh noo'\n",
    "    \"\"\"\n",
    "    return REPEATED_CHARACTER_REGEX.sub(replace_with_double_character, string)\n",
    "\n",
    "\n",
    "def clean_string(string,\n",
    "                 lowercase_characters=True,\n",
    "                 remove_html=True,\n",
    "                 remove_excess_whitespace=True,\n",
    "                 replace_repeated_characters=True,\n",
    "                 remove_urls=True\n",
    "                 ):\n",
    "    if remove_urls:\n",
    "        string = URL_REGEX.sub('', string)\n",
    "    if remove_html:\n",
    "        string = remove_html_from_string(string)\n",
    "    if replace_repeated_characters:\n",
    "        string = remove_repeated_alpha_chars(string)\n",
    "    if remove_excess_whitespace:\n",
    "        string = remove_excess_whitespace_from_string(string)\n",
    "    if lowercase_characters:\n",
    "        string = string.lower()\n",
    "    \n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "local_filename = url.split('/')[-1]\n",
    "print(local_filename)\n",
    "\n",
    "MIN_LINE_LENGTH = 256\n",
    "MAX_LINE_LENGTH = 4096\n",
    "\n",
    "with open_zipped_file(local_filename) as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            jsn = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        if jsn['is_self'] and len(jsn['selftext']) >= MIN_LINE_LENGTH and jsn['subreddit'] != 'AskReddit' \\\n",
    "                          and len(jsn['selftext']) <= MAX_LINE_LENGTH:     \n",
    "            print(jsn['title'])\n",
    "            print(jsn['selftext'])\n",
    "            print(jsn['subreddit'])\n",
    "            print(len(jsn['selftext']))\n",
    "            print(jsn['author'])\n",
    "            print(jsn.keys())\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import pickle \n",
    "\n",
    "\n",
    "author_counts = Counter()\n",
    "\n",
    "for fn in sorted(glob.glob('/data/reddit/*.json')):\n",
    "    jsn = json.load(open(fn))\n",
    "    for d in jsn:\n",
    "        author = d.get('author')\n",
    "        subreddit = d.get('subreddit')\n",
    "        author_counts[(author, subreddit)] += 1\n",
    "        \n",
    "\n",
    "pickle.dump(author_counts, open('/data/reddit/author_counts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "author_counts = pickle.load(open('/data/reddit/author_counts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import langdetect\n",
    "\n",
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def myfunc(row):\n",
    "    selftext = row['selftext']\n",
    "    id_ = row['id']\n",
    "    selftext = clean_string(selftext)\n",
    "    try:\n",
    "        lang = langdetect.detect(selftext[:200])\n",
    "    except Exception:\n",
    "        lang = 'none'\n",
    "    pbar.update(1)\n",
    "    return (id_, lang)\n",
    "\n",
    "ids_langs = []\n",
    "\n",
    "for fn in tqdm(sorted(glob.glob('/data/reddit/*.json'))):\n",
    "    print(fn)\n",
    "    jsn = json.load(open(fn))\n",
    "    #jsn = jsn[:1000]\n",
    "\n",
    "    pool = Pool(30)\n",
    "\n",
    "    arr = pool.imap_unordered(myfunc, jsn)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pbar.close()\n",
    "    \n",
    "    ids_langs += list(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(ids_langs, open('ids_langs.pkl', 'wb'))\n",
    "\n",
    "ids_langs = pickle.load(open('/data/reddit/ids_langs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_lang_ids = set()\n",
    "\n",
    "for id_, lang in ids_langs:\n",
    "    if lang !='en':\n",
    "        bad_lang_ids.add(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016939969456531208"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_lang_ids) / len(ids_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/reddit/downloads/RS_2016-06.json\n",
      "/data/reddit/downloads/RS_2016-07.json\n",
      "/data/reddit/downloads/RS_2016-08.json\n",
      "/data/reddit/downloads/RS_2016-09.json\n",
      "/data/reddit/downloads/RS_2016-10.json\n",
      "/data/reddit/downloads/RS_2016-11.json\n",
      "/data/reddit/downloads/RS_2016-12.json\n",
      "/data/reddit/downloads/RS_2017-01.json\n",
      "/data/reddit/downloads/RS_2017-02.json\n",
      "/data/reddit/downloads/RS_2017-03.json\n",
      "/data/reddit/downloads/RS_2017-04.json\n",
      "/data/reddit/downloads/RS_2017-05.json\n",
      "/data/reddit/downloads/RS_2017-06.json\n",
      "/data/reddit/downloads/RS_2017-07.json\n",
      "/data/reddit/downloads/RS_2017-08.json\n",
      "/data/reddit/downloads/RS_2017-09.json\n",
      "/data/reddit/downloads/RS_2017-10.json\n",
      "/data/reddit/downloads/RS_2017-11.json\n",
      "/data/reddit/downloads/RS_2017-12.json\n",
      "/data/reddit/downloads/RS_2018-01.json\n",
      "/data/reddit/downloads/RS_2018-02.json\n",
      "/data/reddit/downloads/RS_2018-03.json\n",
      "/data/reddit/downloads/RS_2018-04.json\n",
      "/data/reddit/downloads/RS_2018-05.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "MIN_LINE_LENGTH = 256\n",
    "MAX_LINE_LENGTH = 4096\n",
    "\n",
    "def bot_name(author):\n",
    "    author = author.lower()\n",
    "    for s in ['bot', 'mod', 'moderator', 'moderater']:\n",
    "        if author.endswith(s):\n",
    "            return True\n",
    "    if author.startswith('auto'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "subreddit_counter = Counter()\n",
    "raw_dict = defaultdict(list)\n",
    "\n",
    "for fn in sorted(glob.glob('/data/reddit/downloads/*.json')):\n",
    "    print(fn)\n",
    "    jsn = json.load(open(fn))\n",
    "    for d in jsn:\n",
    "        id_ = d['id']\n",
    "        author = d.get('author')\n",
    "        subreddit = d.get('subreddit')\n",
    "        \n",
    "        author_count = author_counts[(author, subreddit)]\n",
    "        \n",
    "        raw_dict['id'].append(id_)\n",
    "        raw_dict['good_lang'].append(not id_ in bad_lang_ids)\n",
    "        raw_dict['author_count'].append(author_count)\n",
    "        raw_dict['bot_name'].append(bot_name(author))\n",
    "        \n",
    "        raw_dict['subreddit'].append(subreddit)\n",
    "        \n",
    "        is_distinguished = d['distinguished'] is not None\n",
    "        raw_dict['is_distinguished'].append(is_distinguished)\n",
    "        \n",
    "        selftext = d['selftext']\n",
    "        selftext = clean_string(selftext)\n",
    "\n",
    "        good_text = len(selftext) >= MIN_LINE_LENGTH and len(selftext) < MAX_LINE_LENGTH\n",
    "        raw_dict['good_text'].append(good_text)\n",
    "\n",
    "good_data_df = pd.DataFrame(raw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_data_df.to_csv('good_data_df.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot_name                       0.039800376285254145\n",
      "good_lang                      0.9830600305434688\n",
      "good_text                      0.9420818143254593\n",
      "is_distinguished               0.014504196844542561\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "good_data_df.columns\n",
    "\n",
    "for c in ['bot_name', 'good_lang', 'good_text',  'is_distinguished',]:\n",
    "    print(c.ljust(30), np.mean(good_data_df[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18661479, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "good_data_df = pd.read_csv('good_data_df.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "MAX_AUTHOR_COUNT = 10\n",
    "\n",
    "subdf = good_data_df[~good_data_df.is_distinguished]\n",
    "subdf = subdf[subdf.good_text]\n",
    "subdf = subdf[subdf.author_count <= MAX_AUTHOR_COUNT]\n",
    "subdf = subdf[subdf.good_lang]\n",
    "subdf = subdf[~subdf.bot_name]\n",
    "\n",
    "counter = Counter(subdf.subreddit)\n",
    "enough_post_subreddits = set([k for k, cnt in counter.most_common() if cnt >= 1000])\n",
    "\n",
    "subdf = subdf[subdf.subreddit.isin(enough_post_subreddits)]\n",
    "\n",
    "subdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdf['id_hash'] = [hash(s) for s in subdf.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we do some subsampling here - this is mainly to help speed up the long deduplication process later\n",
    "\n",
    "SUBSAMPLE_SIZE = 2000\n",
    "\n",
    "def hashed_subsample(subdf_):\n",
    "    subdf_ = subdf_.sort_values('id_hash')\n",
    "    return subdf_.head(SUBSAMPLE_SIZE)\n",
    "\n",
    "subsample_df = subdf.groupby('subreddit').apply(hashed_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e736c2abda4fe39d2292ca39ce4a66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/reddit/downloads/RS_2016-06.json\n",
      "/data/reddit/downloads/RS_2016-07.json\n",
      "/data/reddit/downloads/RS_2016-08.json\n",
      "/data/reddit/downloads/RS_2016-09.json\n",
      "/data/reddit/downloads/RS_2016-10.json\n",
      "/data/reddit/downloads/RS_2016-11.json\n",
      "/data/reddit/downloads/RS_2016-12.json\n",
      "/data/reddit/downloads/RS_2017-01.json\n",
      "/data/reddit/downloads/RS_2017-02.json\n",
      "/data/reddit/downloads/RS_2017-03.json\n",
      "/data/reddit/downloads/RS_2017-04.json\n",
      "/data/reddit/downloads/RS_2017-05.json\n",
      "/data/reddit/downloads/RS_2017-06.json\n",
      "/data/reddit/downloads/RS_2017-07.json\n",
      "/data/reddit/downloads/RS_2017-08.json\n",
      "/data/reddit/downloads/RS_2017-09.json\n",
      "/data/reddit/downloads/RS_2017-10.json\n",
      "/data/reddit/downloads/RS_2017-11.json\n",
      "/data/reddit/downloads/RS_2017-12.json\n",
      "/data/reddit/downloads/RS_2018-01.json\n",
      "/data/reddit/downloads/RS_2018-02.json\n",
      "/data/reddit/downloads/RS_2018-03.json\n",
      "/data/reddit/downloads/RS_2018-04.json\n",
      "/data/reddit/downloads/RS_2018-05.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sample_ids = set(subsample_df['id'])\n",
    "len(sample_ids)\n",
    "\n",
    "sample_jsns = []\n",
    "\n",
    "for fn in tqdm(sorted(glob.glob('/data/reddit/downloads/*.json'))):\n",
    "    print(fn)\n",
    "    jsn = json.load(open(fn))\n",
    "    for d in jsn:\n",
    "        id_ = d['id']\n",
    "        if id_ in sample_ids:\n",
    "            subreddit = d['subreddit']\n",
    "            #assert(subreddit in good_subreddits)\n",
    "            sample_jsns.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame(sample_jsns)\n",
    "\n",
    "print(sample_df.shape)\n",
    "\n",
    "#sample_df.to_csv('/data/reddit/sample_data.zip', index=False, compression='gzip') # pandas struggled to open this\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(sample_df, open('/data/reddit/good_posts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "sample_df = joblib.load(open('/data/reddit/good_posts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subreddit_info = pd.read_csv('subreddit_info.csv')\n",
    "\n",
    "def filter_subreddits(checked_df):\n",
    "    checked_df = checked_df[~checked_df.subreddit.isnull()]\n",
    "\n",
    "    checked_subreddits = {s.replace('www.reddit.com/r/', '') for s in checked_df.subreddit if type(s) == str}\n",
    "    \n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        checked_df = checked_df[~checked_df[f'level{i}'].isin(bad_categories)]\n",
    "    checked_df = checked_df[checked_df['reason_for_exclusion'].isnull()]\n",
    "    \n",
    "    checked_df = checked_df[~checked_df.level2.isnull()]\n",
    "    return checked_df\n",
    "\n",
    "bad_categories = {\n",
    "     'too_regimented',\n",
    "     'too_broad',\n",
    "     'broad',\n",
    "     'game_recruitment',\n",
    "     'requests',\n",
    "     'bot',\n",
    "     'non_english',\n",
    "     'buy/sell',\n",
    "     'r4r',\n",
    "     'exchange',\n",
    "     'city/province', \n",
    "     'commercial',  \n",
    "     'festival',\n",
    "     'unspecific_posts',\n",
    "    }\n",
    "\n",
    "for i in range(1, 4):\n",
    "    for s in bad_categories:\n",
    "        message = f\"excluding {s} in categories\"\n",
    "        bad_rows = (subreddit_info[f'category_{i}'] == s) & (subreddit_info.reason_for_exclusion.isnull())\n",
    "        subreddit_info.loc[bad_rows, 'reason_for_exclusion'] = message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# category specific filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3844498, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge  = subreddit_info[subreddit_info.reason_for_exclusion.isnull()][['subreddit', 'category_1']]\n",
    "sample_df = sample_df.merge(to_merge, on='subreddit')\n",
    "\n",
    "bad_word_dict ={\n",
    "    'video_game' : {\n",
    "             'bug', \n",
    "             'connection', \n",
    "             'patch', \n",
    "             'resolution', \n",
    "             'screen',\n",
    "             'glitch', \n",
    "             'launcher',\n",
    "             'framerate',\n",
    "             'frames',\n",
    "             'fps',\n",
    "             'update',\n",
    "             'crash',\n",
    "             'lobby',\n",
    "             'matchmak',\n",
    "             'latency',\n",
    "             'black screen',\n",
    "             'issue',\n",
    "             'steam',\n",
    "             'desync',\n",
    "             'gtx',\n",
    "             'cheat',\n",
    "             'file',\n",
    "             'download',\n",
    "             'upload',\n",
    "#              'pc',\n",
    "#              'xbox',\n",
    "#              'ps2', \n",
    "#              'ps3', \n",
    "#              'ps4', \n",
    "#              'ios',\n",
    "#              'android',\n",
    "#              'psp',\n",
    "#              'windows'\n",
    "    },\n",
    "    'music' : {\n",
    "        'tour', 'ticket', 'concert', 'show', 'venue'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_text(rd):\n",
    "    return (rd['title'] + ' ' + rd['selftext']).lower()\n",
    "\n",
    "def bad_row(rd):\n",
    "    category = rd['category_1']\n",
    "    if category in bad_word_dict:\n",
    "        \n",
    "        bad_words = bad_word_dict[category]\n",
    "        text = get_text(rd)\n",
    "        return any(s in text for s in bad_words)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "bad_row_bools = sample_df.apply(bad_row, axis=1)\n",
    "sample_df = sample_df[~bad_row_bools]\n",
    "\n",
    "sample_df.shape\n",
    "\n",
    "#(3844498, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3696440, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "subreddit_counter = Counter(list(sample_df.subreddit))\n",
    "good_subreddits = {k for k, v in subreddit_counter.most_common() if v >= 1000}\n",
    "\n",
    "bad_info_rows = ~subreddit_info.subreddit.isin(good_subreddits) & \\\n",
    "                subreddit_info.reason_for_exclusion.isnull()\n",
    "\n",
    "subreddit_info.loc[bad_info_rows, 'reason_for_exclusion'] = \"not enough posts\"\n",
    "\n",
    "sample_df = sample_df.merge(pd.DataFrame({'subreddit' : list(good_subreddits)}), on='subreddit')\n",
    "sample_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(df_):\n",
    "    df_ = df_.sort_values(['subreddit_count', 'subreddit'], ascending=False)\n",
    "    return df_.head(1)\n",
    "\n",
    "subreddit_info['subreddit_count'] = [subreddit_counter[s] for s in subreddit_info.subreddit]\n",
    "subreddit_info['category_3'] = [s if type(s) == str else '' for s in subreddit_info.category_3]\n",
    "subdf = subreddit_info[subreddit_info.reason_for_exclusion.isnull()]\n",
    "subdf = subdf[subdf.subreddit.isin(good_subreddits)]\n",
    "groupby = subdf.groupby(['category_2',], sort=False)['subreddit_count', \n",
    "                                                              'subreddit', \n",
    "                                                              'category_1', \n",
    "                                                              'category_3']\n",
    "\n",
    "\n",
    "in_dataset = groupby.apply(apply)\n",
    "in_dataset = in_dataset.reset_index(drop=False)\n",
    "\n",
    "most_post_subreddits = set(in_dataset.subreddit)\n",
    "\n",
    "\n",
    "topic_to_subreddit = {row[f'category_2'] : row['subreddit'] for row in in_dataset.to_dict(orient='rows')}\n",
    "\n",
    "def apply(row):\n",
    "    if type(row['reason_for_exclusion']) == str or row['subreddit'] in most_post_subreddits :\n",
    "        return row['reason_for_exclusion']\n",
    "    else:\n",
    "        topic = row[f'category_2']\n",
    "        bigger_subreddit = topic_to_subreddit[topic]\n",
    "        \n",
    "        return f\"fewer posts than r/{bigger_subreddit} which shares topic\"\n",
    "\n",
    "    \n",
    "sample_df = sample_df.merge(pd.DataFrame({'subreddit' : list(most_post_subreddits )}), on='subreddit')\n",
    "subreddit_info['reason_for_exclusion'] = subreddit_info.apply(apply, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 1156)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(subreddit_info.reason_for_exclusion.isnull()), len(most_post_subreddits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_text(rd):\n",
    "    out = f\"{rd['title'][:100]} ||| {rd['selftext'][:128]}\"\n",
    "    out = out.replace('\\n', '')\n",
    "    out = out.replace('\\r', '')\n",
    "    out = out.replace('\\t', '')\n",
    "    return out\n",
    "\n",
    "sample_df['text'] = sample_df.apply(join_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# to_write = sample_df[['id', 'text']]\n",
    "# to_write.columns = ['identifier', 'text']\n",
    "\n",
    "# to_write.to_csv(\"reddit_dedupe_input_v2.tsv\", sep='\\t', index=False)\n",
    "\n",
    "# len(set(sample_df.subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicates_df = pd.read_csv('/home/mike/dev/duplicates/reddit_dedupe_output_v2.tsv', sep='\\t')\n",
    "\n",
    "duplicates_df = duplicates_df.drop_duplicates()\n",
    "\n",
    "sample_df = sample_df.merge(duplicates_df, on='text', how='left')\n",
    "\n",
    "text_counter = Counter(sample_df.selftext)\n",
    "\n",
    "sample_df['raw_dups'] = [text_counter[s] for s in sample_df.selftext]\n",
    "sample_df = sample_df[sample_df.dups.isnull()]\n",
    "sample_df = sample_df[sample_df.raw_dups == 1]\n",
    "\n",
    "sample_df = sample_df[['id', 'subreddit', 'title', 'selftext',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1939932, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "subreddit_counter = Counter(list(sample_df.subreddit))\n",
    "good_subreddits = {k for k, v in subreddit_counter.most_common() if v >= 1000}\n",
    "\n",
    "bad_info_rows = ~subreddit_info.subreddit.isin(good_subreddits) & \\\n",
    "                subreddit_info.reason_for_exclusion.isnull()\n",
    "\n",
    "subreddit_info.loc[bad_info_rows, 'reason_for_exclusion'] = \"not enough posts\"\n",
    "\n",
    "sample_df = sample_df.merge(pd.DataFrame({'subreddit' : list(good_subreddits)}), on='subreddit').copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_df['id_hash'] = [hash(s) for s in sample_df['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153000, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSAMPLE_SIZE = 1000\n",
    "\n",
    "def hashed_subsample(subdf_):\n",
    "    subdf_ = subdf_.sort_values('id_hash')\n",
    "    return subdf_.head(SUBSAMPLE_SIZE)\n",
    "\n",
    "sample_df = sample_df.groupby('subreddit').apply(hashed_subsample)\n",
    "\n",
    "sample_df = sample_df[['id', 'subreddit',  'title', 'selftext', ]]\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downsampling video_game subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_game_subreddits = subreddit_info[(subreddit_info.reason_for_exclusion.isnull()) & \\\n",
    "                (subreddit_info.category_1 == 'video_game')].subreddit\n",
    "\n",
    "to_remove = list(sorted(video_game_subreddits, key = hash))[100:]\n",
    "subreddit_info.loc[subreddit_info.subreddit.isin(to_remove), 'reason_for_exclusion'] = \"randomly removed video game subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit_info['in_data'] = subreddit_info.reason_for_exclusion.isnull()\n",
    "subreddit_info = subreddit_info.sort_values(['category_1','category_2','category_3', 'subreddit'])\n",
    "subreddit_info = subreddit_info[['subreddit','category_1','category_2','category_3', \n",
    "                                 'in_data', 'reason_for_exclusion']]\n",
    "\n",
    "subreddit_info.to_csv('subreddit_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_subreddits = subreddit_info[subreddit_info.reason_for_exclusion.isnull()].subreddit\n",
    "\n",
    "sample_df = sample_df[sample_df.subreddit.isin(final_subreddits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(sample_df, test_size=0.2, stratify = list(sample_df.subreddit))\n",
    "sample_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_bad_characters(string):\n",
    "    string = string.replace('\\n', '<lb>')\n",
    "    string = string.replace('\\r', '<lb>')\n",
    "    string = string.replace('\\t', '<tab>')\n",
    "    return string\n",
    "\n",
    "to_write = sample_df.copy()\n",
    "to_write['title'] = [replace_bad_characters(s) for s in to_write.title]\n",
    "to_write['selftext'] = [replace_bad_characters(s) for s in to_write.selftext]\n",
    "\n",
    "to_write.to_csv('rspct.tsv' , sep='\\t', index=False,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
